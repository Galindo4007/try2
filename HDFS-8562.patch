diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/NewFileInputStream.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/NewFileInputStream.java
new file mode 100644
index 0000000..622893b
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/NewFileInputStream.java
@@ -0,0 +1,97 @@
+/*
+ * Copyright (c) 1994, 2013, Oracle and/or its affiliates. All rights reserved.
+ * ORACLE PROPRIETARY/CONFIDENTIAL. Use is subject to license terms.
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ *
+ */
+
+package org.apache.hadoop.hdfs;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import sun.nio.ch.FileChannelImpl;
+
+import java.io.*;
+import java.nio.channels.FileChannel;
+import java.nio.file.Files;
+import java.util.List;
+
+
+public class NewFileInputStream extends InputStream{
+  public static final Log LOG = LogFactory.getLog(NewFileInputStream.class);
+  private final FileDescriptor fd;
+
+  private Closeable parent;
+  private List<Closeable> otherParents;
+
+  private final String path;
+
+  private BufferedInputStream bufferedInputStream;
+  private InputStream inputStream;
+  private FileChannel channel = null;
+
+
+  public NewFileInputStream(File file) throws IOException{
+    LOG.info("++++++++++++++++++++++++++++++++++++++++++NewFileInputStream(File file)++++++++++++++++++++++++++++++++++++++++++++++++++");
+    String name = (file != null ? file.getPath() : null);
+
+    inputStream = Files.newInputStream(file.toPath());
+    fd = new FileDescriptor();
+    path = name;
+    open(name);
+  }
+
+  private void open(String name) throws FileNotFoundException {
+    open0(name);
+  }
+
+  private native void open0(String name) throws FileNotFoundException;
+
+  public FileChannel getChannel() {
+    LOG.info("++++++++++++++++++++++++++++++++++++++++++FileChannel getChannel()++++++++++++++++++++++++++++++++++++++++++++++++++");
+    synchronized (this) {
+      if (channel == null) {
+        channel = FileChannelImpl.open(fd, path, true, false, this);
+      }
+      return channel;
+    }
+  }
+
+  public NewFileInputStream(String name) throws FileNotFoundException,IOException {
+    this(name != null ? new File(name) : null);
+    LOG.info("++++++++++++++++++++++++++++++++++++++++++NewFileInputStream(String name)++++++++++++++++++++++++++++++++++++++++++++++++++");
+  }
+
+  public final FileDescriptor getFD() throws IOException {
+    LOG.info("++++++++++++++++++++++++++++++++++++++++++FileDescriptor getFD()++++++++++++++++++++++++++++++++++++++++++++++++++");
+    if (fd != null) {
+      return fd;
+    }
+    throw new IOException();
+  }
+
+  @Override
+  public int read() throws IOException{
+    LOG.info("++++++++++++++++++++++++++read method +++++++++++++++++++++++++++++++++");
+    bufferedInputStream = new BufferedInputStream(inputStream);
+    return bufferedInputStream.read();
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java
index 4977fd7..9a272f1 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java
@@ -23,7 +23,6 @@
 import java.io.DataOutputStream;
 import java.io.EOFException;
 import java.io.File;
-import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.RandomAccessFile;
 import java.nio.ByteBuffer;
@@ -35,6 +34,7 @@
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.hdfs.DFSUtil;
 import org.apache.hadoop.hdfs.HdfsConfiguration;
+import org.apache.hadoop.hdfs.NewFileInputStream;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.util.DataChecksum;
 
@@ -88,7 +88,7 @@ public static DataChecksum readDataChecksum(File metaFile) throws IOException {
     DataInputStream in = null;
     try {
       in = new DataInputStream(new BufferedInputStream(
-        new FileInputStream(metaFile), DFSUtil.getIoFileBufferSize(conf)));
+        new NewFileInputStream(metaFile), DFSUtil.getIoFileBufferSize(conf)));
       return readDataChecksum(in, metaFile);
     } finally {
       IOUtils.closeStream(in);
@@ -153,7 +153,7 @@ public static BlockMetadataHeader readHeader(File file) throws IOException {
     DataInputStream in = null;
     try {
       in = new DataInputStream(new BufferedInputStream(
-                               new FileInputStream(file)));
+                               new NewFileInputStream(file)));
       return readHeader(in);
     } finally {
       IOUtils.closeStream(in);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
index e265dad..17da2f4 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -100,10 +100,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.StorageType;
 import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hdfs.DFSConfigKeys;
-import org.apache.hadoop.hdfs.DFSUtil;
-import org.apache.hadoop.hdfs.HDFSPolicyProvider;
-import org.apache.hadoop.hdfs.HdfsConfiguration;
+import org.apache.hadoop.hdfs.*;
 import org.apache.hadoop.hdfs.client.BlockReportOptions;
 import org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;
 import org.apache.hadoop.hdfs.net.DomainPeerServer;
@@ -1594,7 +1591,7 @@ public ShortCircuitFdsVersionException(String msg) {
     }
   }
 
-  FileInputStream[] requestShortCircuitFdsForRead(final ExtendedBlock blk,
+  NewFileInputStream[] requestShortCircuitFdsForRead(final ExtendedBlock blk,
       final Token<BlockTokenIdentifier> token, int maxVersion) 
           throws ShortCircuitFdsUnsupportedException,
             ShortCircuitFdsVersionException, IOException {
@@ -1611,10 +1608,10 @@ public ShortCircuitFdsVersionException(String msg) {
         maxVersion);
     }
     metrics.incrBlocksGetLocalPathInfo();
-    FileInputStream fis[] = new FileInputStream[2];
+    NewFileInputStream fis[] = new NewFileInputStream[2];
     
     try {
-      fis[0] = (FileInputStream)data.getBlockInputStream(blk, 0);
+      fis[0] = (NewFileInputStream)data.getBlockInputStream(blk, 0);
       fis[1] = DatanodeUtil.getMetaDataInputStream(blk, data);
     } catch (ClassCastException e) {
       LOG.debug("requestShortCircuitFdsForRead failed", e);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
index 26d669c..6f26951 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
@@ -51,6 +51,7 @@
 import org.apache.hadoop.hdfs.DFSUtil;
 import org.apache.hadoop.hdfs.ExtendedBlockId;
 import org.apache.hadoop.hdfs.HdfsConfiguration;
+import org.apache.hadoop.hdfs.NewFileInputStream;
 import org.apache.hadoop.hdfs.net.Peer;
 import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
 import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
@@ -90,13 +91,19 @@
 import com.google.common.base.Preconditions;
 import com.google.protobuf.ByteString;
 import org.apache.hadoop.util.Time;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 /**
  * Thread for processing incoming/outgoing data stream.
  */
 class DataXceiver extends Receiver implements Runnable {
+
   public static final Log LOG = DataNode.LOG;
+  static{
+    LOG.info("+++++++++++++++++++++++++++DataXceiver++++++++++++++++++++++++++++++");
+  }
   static final Log ClientTraceLog = DataNode.ClientTraceLog;
   
   private Peer peer;
@@ -302,7 +309,7 @@ public void requestShortCircuitFds(final ExtendedBlock blk,
         throws IOException {
     updateCurrentThreadName("Passing file descriptors for block " + blk);
     BlockOpResponseProto.Builder bld = BlockOpResponseProto.newBuilder();
-    FileInputStream fis[] = null;
+    NewFileInputStream fis[] = null;
     SlotId registeredSlotId = null;
     boolean success = false;
     try {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java
index 746c3f6..67e0ebb 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java
@@ -23,6 +23,7 @@
 import java.io.IOException;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hdfs.NewFileInputStream;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;
@@ -127,12 +128,12 @@ public static File idToBlockDir(File root, long blockId) {
    * @throws ClassCastException
    *           if the underlying input stream is not a FileInputStream.
    */
-  public static FileInputStream getMetaDataInputStream(
+  public static NewFileInputStream getMetaDataInputStream(
       ExtendedBlock b, FsDatasetSpi<?> data) throws IOException {
     final LengthInputStream lin = data.getMetaDataInputStream(b);
     if (lin == null) {
       throw new FileNotFoundException("Meta file for " + b + " not found.");
     }
-    return (FileInputStream)lin.getWrappedStream();
+    return (NewFileInputStream)lin.getWrappedStream();
   }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
index d1f7c5f..4eaed85 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
@@ -20,7 +20,6 @@
 import java.io.BufferedInputStream;
 import java.io.DataInputStream;
 import java.io.File;
-import java.io.FileInputStream;
 import java.io.FileNotFoundException;
 import java.io.FileOutputStream;
 import java.io.IOException;
@@ -39,6 +38,7 @@
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.DFSUtil;
+import org.apache.hadoop.hdfs.NewFileInputStream;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;
 import org.apache.hadoop.hdfs.protocol.BlockListAsLongs.BlockReportReplica;
@@ -633,7 +633,7 @@ private long validateIntegrityAndSetLength(File blockFile, long genStamp) {
         return 0;
       }
       checksumIn = new DataInputStream(
-          new BufferedInputStream(new FileInputStream(metaFile),
+          new BufferedInputStream(new NewFileInputStream(metaFile),
               ioFileBufferSize));
 
       // read and handle the common header here. For now just a version
@@ -648,7 +648,7 @@ private long validateIntegrityAndSetLength(File blockFile, long genStamp) {
         return 0;
       }
       IOUtils.skipFully(checksumIn, (numChunks-1)*checksumSize);
-      blockIn = new FileInputStream(blockFile);
+      blockIn = new NewFileInputStream(blockFile);
       long lastChunkStartPos = (numChunks-1)*bytesPerChecksum;
       IOUtils.skipFully(blockIn, lastChunkStartPos);
       int lastChunkSize = (int)Math.min(
@@ -719,9 +719,9 @@ private boolean readReplicasFromCache(ReplicaMap volumeMap,
       }
       return false;
     }
-    FileInputStream inputStream = null;
+    NewFileInputStream inputStream = null;
     try {
-      inputStream = new FileInputStream(replicaFile);
+      inputStream = new NewFileInputStream(replicaFile);
       BlockListAsLongs blocksList =  BlockListAsLongs.readFrom(inputStream);
       Iterator<BlockReportReplica> iterator = blocksList.iterator();
       while (iterator.hasNext()) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java
index f70d4af..b9818d8 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java
@@ -49,6 +49,7 @@
 import org.apache.hadoop.fs.ChecksumException;
 import org.apache.hadoop.hdfs.ExtendedBlockId;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.NewFileInputStream;
 import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;
 import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
 import org.apache.hadoop.hdfs.server.datanode.DatanodeUtil;
@@ -430,7 +431,7 @@ long roundUpPageSize(long count) {
     @Override
     public void run() {
       boolean success = false;
-      FileInputStream blockIn = null, metaIn = null;
+      NewFileInputStream blockIn = null, metaIn = null;
       MappableBlock mappableBlock = null;
       ExtendedBlock extBlk = new ExtendedBlock(key.getBlockPoolId(),
           key.getBlockId(), length, genstamp);
@@ -446,7 +447,7 @@ public void run() {
         }
         reservedBytes = true;
         try {
-          blockIn = (FileInputStream)dataset.getBlockInputStream(extBlk, 0);
+          blockIn = (NewFileInputStream)dataset.getBlockInputStream(extBlk, 0);
           metaIn = DatanodeUtil.getMetaDataInputStream(extBlk, dataset);
         } catch (ClassCastException e) {
           LOG.warn("Failed to cache " + key +
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
index a2bb2c0..b1b398f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
@@ -57,10 +57,7 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.StorageType;
-import org.apache.hadoop.hdfs.DFSConfigKeys;
-import org.apache.hadoop.hdfs.DFSUtil;
-import org.apache.hadoop.hdfs.ExtendedBlockId;
-import org.apache.hadoop.hdfs.HdfsConfiguration;
+import org.apache.hadoop.hdfs.*;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;
 import org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo;
@@ -227,7 +224,7 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)
           NativeIO.getShareDeleteFileInputStream(meta),
           meta.length());
     }
-    return new LengthInputStream(new FileInputStream(meta), meta.length());
+    return new LengthInputStream(new NewFileInputStream(meta), meta.length());
   }
     
   final DataNode datanode;
@@ -978,7 +975,7 @@ private static void computeChecksum(File srcMeta, File dstMeta,
       int offset = 0;
       try (InputStream dataIn = isNativeIOAvailable ?
           NativeIO.getShareDeleteFileInputStream(blockFile) :
-          new FileInputStream(blockFile)) {
+          new NewFileInputStream(blockFile)) {
 
         for (int n; (n = dataIn.read(data, offset, data.length - offset)) != -1; ) {
           if (n > 0) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java
index 45aa364..1a629a1 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java
@@ -21,7 +21,6 @@
 import java.io.BufferedInputStream;
 import java.io.Closeable;
 import java.io.DataInputStream;
-import java.io.FileInputStream;
 import java.io.IOException;
 import java.nio.ByteBuffer;
 import java.nio.MappedByteBuffer;
@@ -32,12 +31,15 @@
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.fs.ChecksumException;
+import org.apache.hadoop.hdfs.NewFileInputStream;
 import org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader;
 import org.apache.hadoop.io.nativeio.NativeIO;
 import org.apache.hadoop.util.DataChecksum;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Preconditions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * Represents an HDFS block that is mmapped by the DataNode.
@@ -45,9 +47,16 @@
 @InterfaceAudience.Private
 @InterfaceStability.Unstable
 public class MappableBlock implements Closeable {
+
+  private static final Logger LOG =
+      LoggerFactory.getLogger(MappableBlock.class);
   private MappedByteBuffer mmap;
   private final long length;
 
+  static {
+    LOG.info("org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MappableBlock+++++++++++++++++++++++++++++++++++++");
+  }
+
   MappableBlock(MappedByteBuffer mmap, long length) {
     this.mmap = mmap;
     this.length = length;
@@ -73,8 +82,9 @@ public long getLength() {
    * @return               The Mappable block.
    */
   public static MappableBlock load(long length,
-      FileInputStream blockIn, FileInputStream metaIn,
+      NewFileInputStream blockIn, NewFileInputStream metaIn,
       String blockFileName) throws IOException {
+    LOG.info("+++++++++++++++++++++++++++++++++++++++load+++++++++++++++++++++++++++++++++++++++++++++");
     MappableBlock mappableBlock = null;
     MappedByteBuffer mmap = null;
     FileChannel blockChannel = null;
@@ -102,7 +112,7 @@ public static MappableBlock load(long length,
    * Verifies the block's checksum. This is an I/O intensive operation.
    */
   private static void verifyChecksum(long length,
-      FileInputStream metaIn, FileChannel blockChannel, String blockFileName)
+      NewFileInputStream metaIn, FileChannel blockChannel, String blockFileName)
           throws IOException, ChecksumException {
     // Verify the checksum from the block's meta file
     // Get the DataChecksum from the meta file header
